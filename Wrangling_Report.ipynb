{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Analyst Nanodegree\n",
    "# **Wrangling Report**\n",
    "---\n",
    "## Project 4: `WeRateDogs` Twitter Analysis - Data Wrangling - Gater, asses, clean and analyse tweets about dog ratings\n",
    "\n",
    ">**`What its all about`**:  \n",
    "The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media 'coverage.\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for you to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017. \n",
    "\n",
    "(cited from Udacity, on January 24, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "---\n",
    "\n",
    "> This notebook contains data from three sources:   \n",
    "1. A given .csv file provided by Udacity (\"twitter-archive-enhanced-2.csv\") containint information on all required tweets, image URLs, tweet_ids, text etc.  \n",
    "2. A .tsv file containing a neural networks prediction on dog breeds (can be downloaded from Udacity as well).  \n",
    "3. A .txt file generated with additional tweet information after scraping them from twitter via the tweepy API and storing information in JSON format.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling: Gather, asses and clean \n",
    "\n",
    "Overall there is a detailed description inside the wrangle-act.ipynb that describes every cleaning step, so to avoid redundancy its only shortly described here in bullet points.\n",
    "---\n",
    "> **`Procedure`**  \n",
    "In this notebook I followed the \"identify-code-test\" procedure, meaning after I identified an issue, I  directly choose an appropriate method to solve it, wrote it out in code and then tested wheter the cleaning has worked. This way I can be sure not to miss anything later.  \n",
    "\n",
    "> The WeRateDogs dataset is a really open-ended collection of tweets and not something to be compared to the statistical test results of a medical trial. Therefore many values/descriptions are having weird values just for fun, that does not necessarily mean to be wrong. For instance the dogs Ratings (13/10) is something very interesting, that in other situations might be considered as totally wrong. Herein however is a kind of running gag to give these funny dogs more points than possible and this even made the WeRateDogs account famous for. So this is something we will definetly not change.\n",
    "1. Initially the workspace contained three dataframes:  \n",
    "    1. `tw_archive` - containing tweet text, rating and dog category.\n",
    "    2. `image_pred` - containing a neural network's prediction on the dog breed.\n",
    "    3. `tweet_df` - containing additional information retweet counts and likes.  \n",
    "2. I first merged these dataframes to avoid repeat cleanining issues on different dataframes multiple times.\n",
    "    \n",
    "> I then went on identifying issues and cleaned them, approximately in this order:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### `tweets` issues\n",
    ">**completeness**  \n",
    "* I did not find any issues of completeness that we could solve. Most of this datset contains text or image data, and in case of missing / NaN values, theres noting we could use like imputing them with a statistical calculation, like the mean.\n",
    "\n",
    ">**data types**   \n",
    "* tweet_id is int64 -> was converted to string\n",
    "* timestamp is type object, convert it to pandas datetime\n",
    "* retweeted_status_timestamp is type object, convert it to pandas datetime\n",
    "* in_reply_to_status_id is int64 -> was succesfully coerced\n",
    "* in_reply_to_user_id is int64 -> was succesfully coerced\n",
    "\n",
    ">**validity / accuracy**  \n",
    "* valid, but maybe wrong dog names: I herein used a trick: I saw that possibly wrong dog names were lowercase, so i selected all rows in the dataset that contained lowercase names, then created a set of possible true names and screened each tweet text if this real name occured. Of curse this is not perfect as dog names might not be in this set already but we could replace around 10 wrong dog names with their repsective true names. \n",
    "\n",
    "\n",
    "#### `preds` issues\n",
    ">**completeness**  \n",
    " * missing data, not restorable as neural net is not provided\n",
    "\n",
    ">**tidiness**  \n",
    "* dog (1-3) and conf (1-3) are individual columns, melt down\n",
    "* tweet_id is int64 -> was successfully coerced\n",
    "\n",
    "#### `add_info` issues\n",
    ">**completeness**    \n",
    "* missing data for 25 tweets -> drop \n",
    "\n",
    ">**data types**   \n",
    "* tweet_id is int64 -> was successfully coerced\n",
    "\n",
    "#### `global tidiness `\n",
    "* currently we are having three tables. These can be converted into one master df. As some issues that would need to be cleaned are similar / same accross these three datasets, it makes sense to first merge them and then address all issues on one single dataframe instead of repeting it thrice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "---\n",
    "In the end I saved the most important file as `twitter_archive_master.csv`. This file can be used for most of the analyses and plots (sometimes requiring some minor modifications.\n",
    "\n",
    "For learning purposes I also stored some data in a SQLite database and saved some other intermediate files as .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining problems\n",
    "---\n",
    "I identified some issues, that might be solved but did not cause any problem so I kept them as they are:\n",
    "1. The tweet text is somewhat cut off after a few words.\n",
    "2. Some information (like retweet_status etc is not useful). We might want to drop it in order to decrease the memory usage, but the entire dataset is not too large to block the computer from running efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
